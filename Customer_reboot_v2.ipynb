{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Customer_reboot_training_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target0, target1 = list(data['reset'].value_counts())\n",
    "total = target0 + target1\n",
    "print(f'Percentage of non reboots: {target0*100/total}')\n",
    "print(f'Percentage of reboots: {target1*100/total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error preprocessing\n",
    "Some errors from wifi boxes where packet counts are misreported (eg. packetloss > 2billion) were apparent in this dataset, and so the 95th percentile was taken in order to combat this reporting error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PktLoss_perc95 = np.percentile(data['med_PktLoss_2_4'], 95)\n",
    "fdata = data[data.med_PktLoss_2_4 <= PktLoss_perc95]\n",
    "UPkt_perc95 = np.percentile(fdata['med_UPkts_2_4'], 95)\n",
    "fdata = fdata[fdata.med_UPkts_2_4 <= UPkt_perc95]\n",
    "MPkt_perc95 = np.percentile(fdata['med_MPkts_2_4'], 95)\n",
    "fdata = fdata[fdata.med_MPkts_2_4 <= MPkt_perc95]\n",
    "\n",
    "percentile_cols = ['med_PktLoss_2_4', 'med_UPkts_2_4', 'med_MPkts_2_4']\n",
    "for col in percentile_cols:\n",
    "    plt.hist(fdata[col])\n",
    "    plt.title(str(col))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classes\n",
    "Downsampling was done in order to combat extreme downsampling, this was taken down to a mild imbalance of ~25/75 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target0, target1 = list(fdata['reset'].value_counts())\n",
    "total = target0 + target1\n",
    "print(f'Percentage of non reboots: {target0*100/total}')\n",
    "print(f'Percentage of reboots: {target1*100/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maj = fdata[fdata.reset==0]\n",
    "data_min = fdata[fdata.reset==1]\n",
    "data_maj_downsampled = resample(data_maj, replace=False, n_samples=target1*3, random_state=123)\n",
    "data_downsampled = pd.concat([data_maj_downsampled, data_min])\n",
    "target0, target1 = list(data_downsampled['reset'].value_counts())\n",
    "total = target0 + target1\n",
    "print(f'Percentage of non reboots: {target0*100/total}')\n",
    "print(f'Percentage of reboots: {target1*100/total}')\n",
    "print(f'Size of dataset: {data_downsampled.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_downsampled.columns)\n",
    "feature_cols = cols[1:]\n",
    "xdata = data_downsampled[feature_cols]\n",
    "target = cols[0]\n",
    "ydata = data_downsampled[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data):\n",
    "    cm = data.corr()\n",
    "    mask = np.triu(cm)\n",
    "    sns.heatmap(cm, mask=mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(data_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downsampled.drop(['Enable_2_4'], axis=1, inplace=True)\n",
    "cols = list(data_downsampled.columns)\n",
    "feature_cols = cols[1:]\n",
    "xdata = data_downsampled[feature_cols]\n",
    "ydata = data_downsampled[target]\n",
    "heatmap(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "Decision tree classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(xdata, ydata, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target0, target1 = list(y_train.value_counts())\n",
    "total = target0 + target1\n",
    "print(f'Percentage of non reboots in training set: {target0*100/total}')\n",
    "print(f'Percentage of reboots in training set: {target1*100/total}')\n",
    "print()\n",
    "target0, target1 = list(y_test.value_counts())\n",
    "total = target0 + target1\n",
    "print(f'Percentage of non reboots in testing set: {target0*100/total}')\n",
    "print(f'Percentage of reboots in testing set: {target1*100/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "y_prob = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics and checking overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.matshow(cm, cmap='RdYlGn')\n",
    "    plt.xlabel('True Class')\n",
    "    plt.ylabel('Predicted Class')\n",
    "    for (x, y), value in np.ndenumerate(cm):\n",
    "        plt.text(x, y, f\"{value }\", va=\"center\", ha=\"center\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "conf_mat(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix:\n",
    "True negatives: 6058\n",
    "\n",
    "False negatives: 17\n",
    "\n",
    "False positives: 68\n",
    "\n",
    "True positives: 2049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "feature_cols = list(xdata.columns)\n",
    "\n",
    "plt.bar(feature_cols, importance)\n",
    "plt.title('Feature Importance on Downsampled Data')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.array(range(1,20))\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i in depth:\n",
    "\tmodel = DecisionTreeClassifier(max_depth=i)\n",
    "\tmodel.fit(x_train, y_train)\n",
    "\n",
    "\ty_pred_train = model.predict(x_train)\n",
    "\ttrain_acc = accuracy_score(y_train, y_pred_train)\n",
    "\ttrain_scores.append(train_acc)\n",
    "\n",
    "\ty_pred_test = model.predict(x_test)\n",
    "\ttest_acc = accuracy_score(y_test, y_pred_test)\n",
    "\ttest_scores.append(test_acc)\n",
    "\n",
    "plt.plot(depth, train_scores, '-x', label='Train')\n",
    "plt.plot(depth, test_scores, '-x', label='Test')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model integrity \n",
    "This accuracy seems to high, so I used a kfold cross validation and then removed the most important feature in order to see how this impacts the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strKf = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = strKf.split(xdata, ydata)\n",
    "scores = []\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    x_train = xdata.iloc[train, :]\n",
    "    y_train = ydata.iloc[train]\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(xdata.iloc[test, :], ydata.iloc[test])\n",
    "    scores.append(score)\n",
    "    print(f'Fold {k+1}: Training/Test Split Distribution: {np.bincount(ydata.iloc[train])}, Accuracy: {score.round(3)}')\n",
    "\n",
    "print()\n",
    "print(f'Cross validation accuracy: {np.mean(scores).round(3)} +/- {np.std(scores).round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold accuracy\n",
    "Cross validation accuracy: 0.993 +/- 0.0012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.drop(['med_MPkts_2_4'], axis=1, inplace=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(xdata, ydata, test_size=0.3, random_state=123)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_mat(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "feature_cols = list(xdata.columns)\n",
    "\n",
    "plt.bar(feature_cols, importance)\n",
    "plt.title('Feature Importance on Downsampled Data')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fec3ab36716fe125c1a0073683c6433cb45ee410fe515abe8040210eb6e3016"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
