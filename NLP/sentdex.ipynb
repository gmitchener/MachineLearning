{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - sentdex\n",
    "\n",
    "## Tokenizing words and sentences\n",
    "**Tokenizing:**\n",
    "* word tokenizers\n",
    "* sentence tokenizers \n",
    "\n",
    "**Lexicon and corperas:**\n",
    "* corperas - body of text eg. medical journals, presidential speeches, english langauge\n",
    "* lexicon - words and their meanings\n",
    "    * eg. investor speak vs. regular english speak  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard.']\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = 'Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not eat cardboard.'\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = 'This is an example showing off stop word filtration.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common one liner for this block of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Different variations of words that carry the same meaning:\n",
    "* I was taking a ride in the car.\n",
    "* I was riding in the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "example_words = ['python', 'pythoner', 'pythoning', 'pythoned', 'pythonly']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is veri import to be pythonli while python with python . all python have python poorli at least onc . "
     ]
    }
   ],
   "source": [
    "new_text = 'It is very important to be pythonly while pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w), sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech (POS) Tagging \n",
    "Labelling the part of speech to every word.\n",
    "\n",
    "List of POS tag list at: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking \n",
    "The next step to figuring out the meaning of the sentence is to chunk uses part of speech tags and regular expressions. \n",
    "\n",
    "### Regular Expressions\n",
    "\n",
    "https://www.tutorialspoint.com/python/python_reg_expressions.htm\n",
    "\n",
    "\n",
    "A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding this regular expression\n",
    "*r*'''Chunk: {<**RB.?**>*<**VB.?**><**NNP**>}'''\n",
    "\n",
    "the period (.) denotes any character except for a new line, and question mark (?) denotes 0 or 1 as no part of speech tag is longer than 3 characters, and possesive pronouns can be. \n",
    "\n",
    "the asterix (*) means 0 or more so this chunk will chunk any form of adverb.\n",
    "\n",
    "the plus (+) means one or more NNP. \n",
    "\n",
    "and a plausible (?) NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking \n",
    "A chink is what we wish to remove from the chunk (you chink from a chunk). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r'''Chunk: {<.*>+} \n",
    "                                    }<VB.?|IN|DT|TO>+{'''\n",
    "\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            print(chunked)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "The NLTK library of named entitiy recognition is not amazing so might need other proceses in place to catch them.\n",
    "\n",
    "Using binary=*True* means that we don't care what the named entity is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            named_ent = nltk.ne_chunk(tagged)\n",
    "            #named_ent = nltk.ne_chunk(tagged, binary=True)\n",
    "\n",
    "            print(named_ent)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "This is similar to stemming, but the end result is a real word. \n",
    "\n",
    "The default POS for lemmatizing is a noun, so if you have something that is not a noun you have to pass through the POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize('cats'))\n",
    "print(lem.lemmatize('cacti'))\n",
    "print(lem.lemmatize('geese'))\n",
    "print(lem.lemmatize('rocks'))\n",
    "print(lem.lemmatize('python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize('better'))\n",
    "print(lem.lemmatize('better', pos='a'))\n",
    "print(lem.lemmatize('best', pos='a'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7907318c2678644a2c014b9018cb77fdbde3102e73d33f90e5edf5bfc480bfb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
